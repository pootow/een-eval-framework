# Basic evaluation configuration for een_eval framework
# This example shows a simple setup using the actual working API

dataset:
  type: "file" 
  path: "data/eval_dataset.jsonl"
  # Optional parameters
  params:
    save: true

models:
  - name: "gpt-4"
    type: "openai"
    config:
      model: "gpt-4"
      temperature: 0.0
      max_tokens: 512
      # Add your API key via environment variable: OPENAI_API_KEY
  
  - name: "gpt-3.5-turbo"
    type: "openai"
    config:
      model: "gpt-3.5-turbo"
      temperature: 0.0
      max_tokens: 512

# Custom evaluation method using function defined in separate file
evaluation_methods:
  - name: "exact_match_evaluation"
    type: "custom"
    path: "eval.py"
    function_name: "evaluate_exact_match"
    params:
      case_sensitive: false
  
  - name: "contains_evaluation"
    type: "custom"
    path: "eval.py"
    function_name: "evaluate_contains"
    params:
      case_sensitive: false

# Metrics similar to prod_qa_params
metrics:
  - name: "pass@1"
    type: "pass_at_k"
    facets:
      - model_name
    params:
      k: 1
  
  - name: "accuracy"
    type: "custom"
    path: "eval.py"
    function_name: "calculate_accuracy"

# Prompt template - can be inline or from file
prompt_template:
  path: "prompt_template.md"

# Global sampling parameters
sample_params:
  temperature: 0.0
  max_tokens: 512
  num_samples: 1

# Output configuration  
output_dir: "results/basic_example"

# Workflow settings
workflow:
  mode: "inference"  # "inference" or "evaluation"
  batch_size: 4
  max_workers: 2

# Resume capability
resume: false

# Logging configuration
logging:
  level: "INFO"
  # file: "eval.log"
