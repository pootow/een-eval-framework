# VLLM configuration for local model evaluation
# This example shows how to use local models with llama.cpp integration

dataset:
  type: "file"
  path: "data/coding_eval.jsonl"
  params:
    save: true

models:
  - name: "Qwen3-1.7B"
    type: "llama.cpp"
    model_path: "unsloth/Qwen3-1.7B-GGUF/Qwen3-1.7B-UD-Q4_K_XL.gguf"
    endpoint: "http://localhost:28000/v1"
    docker:
      models_volume: "F:\\ai-models\\LLMs-LMStudio"
      host_port: 1236
      n_gpu_layers: 99
      no_mmap: true
      context_size: 8000
    inference:
      think: false
  
  - name: "llama2-7b-local"
    type: "llama.cpp"
    model_path: "meta-llama/Llama-2-7b-chat-hf"
    endpoint: "http://localhost:28001/v1"
    docker:
      models_volume: "/path/to/models"
      host_port: 1237
      n_gpu_layers: 32
      context_size: 4096

# Custom evaluation method
evaluation_methods:
  - name: "code_evaluation"
    type: "custom"
    path: "eval.py"
    function_name: "evaluate_code_correctness"
    params:
      timeout: 10
      safe_mode: true
  
  - name: "qa_evaluation"  
    type: "custom"
    path: "eval.py"
    function_name: "evaluate_qa_responses"

# Metrics for comprehensive evaluation
metrics:
  - name: "pass@1[mean@16]"
    type: "pass_at_k"
    facets:
      - model_name
    params:
      k: 1
      num_trials: 16
  
  - name: "pass@16"
    type: "pass_at_k"
    facets:
      - model_name
    params:
      k: 16
  
  - name: "Classification Report"
    type: "classification"
    facets:
      - model_name
    params:
      num_trials: 16

# Prompt template
prompt_template:
  path: "code_prompt_template.md"

# Global sampling parameters for generating multiple samples
sample_params:
  temperature: 0.1
  max_tokens: 2048
  num_samples: 16  # Generate 16 samples for pass@16 evaluation

# Output configuration
output_dir: "output/vllm_evaluation"

# Workflow settings
workflow:
  mode: "inference"
  batch_size: 1
  max_workers: 1

# Resume capability
resume: true

# Logging configuration
logging:
  level: "DEBUG"
  file: "vllm_eval.log"